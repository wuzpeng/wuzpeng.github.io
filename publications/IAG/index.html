<head>
    <!-- Google tag (gtag.js) -->
     <script async src="https://www.googletagmanager.com/gtag/js?id=G-DB5J2MQV0D"></script>
     <script>
     window.dataLayer = window.dataLayer || [];
     function gtag(){dataLayer.push(arguments);}
     gtag('js', new Date());
 
     gtag('config', 'G-DB5J2MQV0D');
     </script>
 
     <script src="http://www.google.com/jsapi" type="text/javascript"></script>
     <script type="text/javascript">
     google.load("jquery", "1.3.2");
     </script>    
 </head>
 
 <style type="text/css">
 body {
     font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
     font-weight: 300;
     font-size: 18px;
     margin-left: auto;
     margin-right: auto;
     width: 1100px;
 }
 
 h1 {
     font-weight: 300;
     margin: 0.4em;
 }
 
 /* p {
     margin: 0.2em;
 } */
 
 .disclaimerbox {
     background-color: #eee;
     border: 1px solid #eeeeee;
     border-radius: 10px;
     -moz-border-radius: 10px;
     -webkit-border-radius: 10px;
     padding: 20px;
 }
 
 video.header-vid {
     height: 140px;
     border: 1px solid black;
     border-radius: 10px;
     -moz-border-radius: 10px;
     -webkit-border-radius: 10px;
 }
 
 img.header-img {
     height: 140px;
     border: 1px solid black;
     border-radius: 10px;
     -moz-border-radius: 10px;
     -webkit-border-radius: 10px;
 }
 
 img.rounded {
     border: 1px solid #eeeeee;
     border-radius: 10px;
     -moz-border-radius: 10px;
     -webkit-border-radius: 10px;
 }
 
 a:link,
 a:visited {
     color: #1367a7;
     text-decoration: none;
 }
 
 a:hover {
     color: #208799;
 }
 
 td.dl-link {
     height: 160px;
     text-align: center;
     font-size: 22px;
 }
 
 .layered-paper-big {
     /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
     box-shadow:
         0px 0px 1px 1px rgba(0, 0, 0, 0.35),
         /* The top layer shadow */
         5px 5px 0 0px #fff,
         /* The second layer */
         5px 5px 1px 1px rgba(0, 0, 0, 0.35),
         /* The second layer shadow */
         10px 10px 0 0px #fff,
         /* The third layer */
         10px 10px 1px 1px rgba(0, 0, 0, 0.35),
         /* The third layer shadow */
         15px 15px 0 0px #fff,
         /* The fourth layer */
         15px 15px 1px 1px rgba(0, 0, 0, 0.35),
         /* The fourth layer shadow */
         20px 20px 0 0px #fff,
         /* The fifth layer */
         20px 20px 1px 1px rgba(0, 0, 0, 0.35),
         /* The fifth layer shadow */
         25px 25px 0 0px #fff,
         /* The fifth layer */
         25px 25px 1px 1px rgba(0, 0, 0, 0.35);
     /* The fifth layer shadow */
     margin-left: 10px;
     margin-right: 45px;
 }
 
 
 .layered-paper {
     /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
     box-shadow:
         0px 0px 1px 1px rgba(0, 0, 0, 0.35),
         /* The top layer shadow */
         5px 5px 0 0px #fff,
         /* The second layer */
         5px 5px 1px 1px rgba(0, 0, 0, 0.35),
         /* The second layer shadow */
         10px 10px 0 0px #fff,
         /* The third layer */
         10px 10px 1px 1px rgba(0, 0, 0, 0.35);
     /* The third layer shadow */
     margin-top: 5px;
     margin-left: 10px;
     margin-right: 30px;
     margin-bottom: 5px;
 }
 
 .vert-cent {
     position: relative;
     top: 50%;
     transform: translateY(-50%);
 }
 
 hr {
     margin: 0;
     border: 0;
     height: 1.5px;
     background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
 }
 
 .rotate {
     /* FF3.5+ */
     -moz-transform: rotate(-90.0deg);
     /* Opera 10.5 */
     -o-transform: rotate(-90.0deg);
     /* Saf3.1+, Chrome */
     -webkit-transform: rotate(-90.0deg);
     /* IE6,IE7 */
     filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
     /* IE8 */
     -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
     /* Standard */
     transform: rotate(-90.0deg);
 }
 
 c {
     white-space: nowrap;
     writing-mode: tb-rl;
     transform: rotate(-180.0deg);
 }
 
     .topnav {
       background-color: #eeeeee;
       overflow: hidden;
     }
 
     .topnav div {
       max-width: 1070px;
       margin: 0 auto;
     }
 
     .topnav a {
       display: inline-block;
       color: black;
       text-align: center;
       vertical-align: middle;
       padding: 16px 16px;
       text-decoration: none;
       font-size: 16px;
     }
 
     .topnav img {
       width: 100%;
       margin: 0.2em 0px 0.3em 0px;
     }
     .authors div{
         text-align: center;
     }
     .content{
         margin-bottom: 2em;
         font-size: 12pt;
     }
     p {
         display: block;
         margin-block-start: 1em;
         margin-block-end: 1em;
         margin-inline-start: 0px;
         margin-inline-end: 0px;
     }
 
 
 </style>
 <html>
 
 <head>
     <title>IAGNet</title>
     <meta property="og:title" content="nlos" />
 </head>
 
 <body>	
 
     <br>
     <center>
     <p>
         <span style="font-size:42px"> Grounding 3D Object Affordance from <br>
             2D Interactions in Images </span>
     </p>
     <p>
         <span style="font-size:22px"> ICCV 2023</span>
     </p>
     </center>
     <br>
     <div  align=center class="authors">
        <a href=""> Yuhang Yang<sup>1</sup></a> 
        &nbsp;  
        <a href=""> Wei Zhai<sup>1</sup></a> 
        &nbsp;  
        <a href=""> Hongchen Luo<sup>1</sup></a> 
        &nbsp;  
        <a href=""> Yang Cao<sup>1,2</sup></a> 
        <br>
        <a href=""> Jiebo Luo<sup>3</sup></a> 
        &nbsp;  
        <a href=""> Zheng-Jun Zha<sup>1</sup></a> 
       
     </div>
     <br>
     <!-- <div align=center class="authors">
         University of Science and Technology of China<sup>1</sup> 
         Google<sup>2</sup> 
         NVIDIA<sup>3</sup>
          
 
     </div> -->
     <div> 
         <table align=center width=800px>
             <tr>
                 <td style="text-align: center;">
                     University of Science and Technology of China<sup>1</sup>  &nbsp; &nbsp; University of Rochester<sup>3</sup>
                 </td>
 
             </tr>
             <tr>
                 <td style="text-align: center;">
                     Institute of Artificial Intelligence, Hefei Comprehensive National Science Center<sup>2</sup> 
                 </td>
             </tr>
         </table>
        
     </div>
 
     
     <br>
     <table align=center width=500px>
         <tr>
             <td align=center width=50px>
                 <center>
                     <a href="https://arxiv.org/pdf/2303.10437.pdf">[Paper]</a>
                 </center>
             </td>
             <td align=center width=50px>
                 <center>
                     <span > <a href="https://github.com/yyvhang/IAGNet">[Code] </a> </span> 
                 </center>
             </td>
             <td align=center width=100px>
                 <center>
                     <span > [Data] release after the conference </span> 
                 </center>
             </td>
         </tr>
     </table>
     <br>
     <br>
     <!-- <hr> -->
     <table align=center width=800px>
     <tr><td>
  
     <div  align=center class="content" width=200px>
     <img height=300 src="assets/intro.png"> </img>
     <br>
     <p align="justify">
             Grounding 3D object affordance seeks to locate objects' ``action possibilities'' regions in the 3D space, which serves as a link 
             between perception and operation for embodied agents. Existing studies primarily focus on connecting visual affordances with geometry structures, 
             e.g., relying on annotations to declare interactive regions of interest on the object and establishing a mapping between the regions and affordances. 
             However, the essence of learning object affordance is to understand how to use it, and the manner that detaches interactions is limited in generalization. 
             Normally, humans possess the ability to perceive object affordances in the physical world through demonstration images or videos. Motivated by this, we 
             introduce a novel task setting: grounding 3D object affordance from 2D interactions in images, which faces the challenge of anticipating affordance through 
             interactions of different sources. To address this problem, we devise a novel Interaction-driven 3D Affordance Grounding Network (IAG), which aligns the 
             region feature of objects from different sources and models the interactive contexts for 3D object affordance grounding. Besides, we collect a Point-Image 
             Affordance Dataset (PIAD) to support the proposed task. Comprehensive experiments on PIAD demonstrate the reliability of the proposed task and the superiority of our method.
     </p>
     </div>
     </td>  </tr>
     </table>
     
     <center>
         <h1>Motivation</h1>
     </center>  
     <hr>
     <table  align=center width=800px>
         <tr>
         <td>
             <img align="center" width="800px" src="assets/motivation.png"></img>
         </td>
         </tr>
         <tr>
         <td>
             <p>
                 This challenging task includes several essential issues that should be properly addressed. <b>1) Alignment ambiguity</b>. 
                 To ground 3D object affordance from 2D source interactions, the premise is to correspond the regions of the object 
                 in different sources. The object in 2D demonstrations and the 3D object faced are usually derived from different 
                 physical instances in different locations and scales. This discrepancy may lead to confusion about corresponding 
                 affordance regions, causing alignment ambiguity. While objects are commonly designed to satisfy certain needs of 
                 human beings, so the same category generally follows a similar combination scheme of object components to meet 
                 certain affordances, and these affordances are hinted by some structures (Fig. 2 (a)). These invariant properties 
                 are across instances and could be utilized to correlate object regions from different sources. <b>2) Affordance ambiguity</b>. 
                 Affordance has properties of dynamic and  multiplicity, which means the object affordance may change according to the 
                 situation, the same part of an object could afford multiple interactions, as shown in Fig. 2 (b), ``Chair'' affords 
                 ``Sit'' or ``Move'' depends on the human actions, ``Mug'' affords ``Wrapgrasp'' or ``Contain'' according to the scene 
                 context, these properties may make ambiguity when extracting affordance. However, these dynamic factors for affordance 
                 extraction can be decomposed into the interaction between subject-object and object-scene. Modeling these interactions 
                 is possible to extract explicit affordance.
             </p>
         </td>
         </tr>
 
     </table>
     <center>
         <h1>Grounding 3D Object Affordance through 2D Interaction</h1>
     </center>
     <table align="center" width=1000px>
         <tr>
         <td colspan='2'>
             <center>
             <video width="500" controls muted autoplay loop>
                 <source src="./assets/video/Chair.mp4" type="video/mp4">
             </video>
             </center>
         </td>
         <td>
             <center>
                 <video width="500"  controls muted autoplay loop>
                 <source src="./assets/video/Bed.mp4" type="video/mp4">
             </video>
             </center>
         </td> 
         </tr>
 
         <tr>
             <td colspan='2'>
                 <center>
                 <video width="500" controls muted autoplay loop>
                     <source src="./assets/video/Faucet.mp4" type="video/mp4">
                 </video>
                 </center>
             </td>
             <td>
                 <center>
                     <video width="500"  controls muted autoplay loop>
                     <source src="./assets/video/Knife.mp4" type="video/mp4">
                 </video>
                 </center>
             </td> 
             </tr>
     </table>
     <table align="center" width=1000px>
         <tr>
             <td>
                 <p>
                 Given an image with specific interactive semantics, the model could ground corresponding affordance regions on different 3D object instances.
                 </p>
             </td>
         </tr>
     </table>
 
     <center>
         <h1>Method</h1>
     </center>  
     <hr>
     <table align=center width=1000px>
         <tr>
             <td>
                 <img align="center" width="1000px" src="assets/method.png"></img>
             </td>
         </tr>
     </table>
     <center>
         <h1>PIAD Data Distribution</h1>
     </center>  
     <hr>
 
     <table align=center width=1000px>
         <tr>
             <td>
                 <img align="center" width="1000px" src="assets/PIAD.png"></img>
             </td>
         </tr>
     </table>
 
     <center>
         <h1>Real World Generalization</h1>
     </center>  
     <hr>
     <table align=center width=800px>
         <tr>
             <td>
                 <img align="center" width="800px" src="assets/real_word.png"></img>
             </td>
         </tr>
         <tr>
             <td>
                 <p>
                     To validate the model in real-world scenarios, we use an iPhone 13 pro to scan objects (the first row) and real-scan dataset 
                     AKB48 (the second row) to test our model, shown in the above figure. It shows that our model exhibits a certain degree 
                     of generalization to the real world.
                 </p>
             </td>
         </tr>
     </table>
 
     <center>
     <p> 
          <span style="font-size:20px">
             Cite Our Paper: <a href="./assets/bibtex.txt"> [BibTex] </a>
         </span>
         
     </p>
     </center>
 
     <center>
         <h1>Contact Us</h1>
     </center>
     <hr>
     <center>
         <p> 
              <span style="font-size:15px">
                 If you have any questions or cooperation intentions, feel free to email yyuhang@mail.ustc.edu.cn
             </span>
             
         </p>
     </center>
 
     <table align="center">
         <tr> <td><p>
             ...
         </p>
      </td></tr>
     </table>
 
     <!-- <center>
         <h1>Paper</h1>
     </center>
     <hr>
     <br>
     <table align="center" width=800px>
         <tr>
             <td colspan="2">
                 <img class="layered-paper-big" width=150px src="assets/paper.png"/>
             </td>
             <td colspan="2">
                 <b>NIFTY: Neural Object Interaction Fields<br> 
                 for Guided Human Motion Synthesis</b> <br>  <br>
                 Kulkarni. N, Rempe. D, Genova K., Kundu A., <br>  Johnson J., Fouhey D., Guibas. L
                 <br> <br>
                 Arxiv, 2023 <br> <br>
                 <a href="./assets/paper.pdf">[Paper] </a>  <a href="./assets/bibtex.txt"> [BibTex] </a>
 
 
             </td>
         </tr>
     </table>
     <br> -->
 
     <!-- <center>
         <h1>Acknowledgments</h1>
     </center>
     <hr>
     <p>
         We express our gratitude to our colleagues for the fantastic project discussions and feedback provided at different stages. We have organized them by institution (in alphabetical order) .
         <ul> 
             <li><i>Google</i> : Matthew Brown, Frank Dellaert, Thomas A. Funkhouser, Varun Jampani</li>
             <li><i>Google (co-interns)</i> : Songyou Peng, Mikaela Uy, Guandao Yang, Xiaoshuai Zhang </li> 
             <li><i>University of Michigan </i> :  Mohamed El Banani, Ang Cao,  Karan Desai,  Richard Higgins,  Sarah Jabbour, Linyi Jin, Jeongsoo Park, Chris Rockwell, Dandan Shan</li>
         </ul>
 
         This work was partly done when NK was interning at Google Research. DR was supported by the NVIDIA Graduate Fellowship. This project page template is based on <a href="https://research.nvidia.com/labs/toronto-ai/trace-pace/"> this page</a>.
     </p>
     <hr>
     <br>
     <table align="center">
         <tr> <td><p>
             ...
         </p>
      </td></tr>
     </table> -->
     
 
 </body>
 
 </html>